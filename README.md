# pubcrawler

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Description

`pubcrawler` downloads all files of a specified type from the subpages
of a website and then extracts metadata from them using LLMs. At this
stage the primary use-case is downloading PDFs from think tanks/policy
organisations and mapping authorship, publishing output and
institutional affliations.

## How to use

### Prerequisites

You will need OpenAI API key to run all scripts. The key must be stored
within the .env file in this directory.

To explore the research output for an institution of your choosing, you
will need to adjust the values in `const` as follows:

``` python
url = 'https://autonomy.work/' # organisation's main page
directory_name = 'autonomy' # name of folder to save all output to 
file_type = 'pdf' # filetype to download
model = 'gpt-3.5-turbo' # openai model for processing text
org_names = ['autonomy'] # alternative names for organisation used to filter out irrelevant results
```

``` python
import pubcrawler as proj
```

#### Execute core scripts

``` python
proj.core.run_all()
```

#### Command-line interface

*Note: You need to activate the conda environment first.*

List of commands

``` python
!python -m pubcrawler.cli
```

Execute commands

``` python
!python -m pubcrawler.cli.core
```

You can find the manual for each command using `-h`
